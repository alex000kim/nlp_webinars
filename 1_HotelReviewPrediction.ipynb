{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-packet",
   "metadata": {},
   "source": [
    "# ‚ú®Learning objectives\n",
    "- Learn how to prepare text data for predictive modelling. We'll compare two techniques: TF-IDF and word2vec\n",
    "- Train and compare the performance of two predictive models: linear regression (from `sklearn` library) and gradient boosting tree regression (from `xgboost` library)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a997d7",
   "metadata": {},
   "source": [
    "# üíº Business question:\n",
    "Can we predict what the rating is going to be based on the text of the review? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfb4fc",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "The source for the dataset: https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews\n",
    "> Hotels play a crucial role in traveling and with the increased access to information new pathways of selecting the best ones emerged.\n",
    "With this dataset, consisting of 20k reviews crawled from Tripadvisor, you can explore what makes a great hotel and maybe even use this model in your travels!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "signed-harrison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/tripadvisor_hotel_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reverse-syndicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20491, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da8963",
   "metadata": {},
   "source": [
    "#### üí°Tip\n",
    "\n",
    "Early splitting of the dataset into train/test (and validation if needed for hyperparameter tuning) reduces the risk of unintentional information leaking between different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "refined-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['Rating'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c5d53",
   "metadata": {},
   "source": [
    "Confirm that distribution of ratings is the same bet ween train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bigger-borough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.444625\n",
       "4    0.291059\n",
       "3    0.106910\n",
       "2    0.088626\n",
       "1    0.068779\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advance-rhythm",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.433535\n",
       "4    0.305680\n",
       "3    0.105602\n",
       "2    0.084130\n",
       "1    0.071052\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd509899",
   "metadata": {},
   "source": [
    "### Transform text with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe0f8a",
   "metadata": {},
   "source": [
    "#### üìù Note\n",
    "\n",
    "As a quick refresher on TF-IDF:\n",
    "- It's one of the ways of vectorizing a text document, i.e. turning words into an array of numbers. Each dimension (i.e. the place in an array) produced by TF-IDF corresponds to a particular word\n",
    "- It's a measure of meaningfulness/originality of a word \n",
    "- It's the ratio between 1) how frequently a word appears in a particular document and 2) how frequently a word appears in _all_ documents combined together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eleven-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF from sklearn has several hyperparameters:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c66eb9",
   "metadata": {},
   "source": [
    "#### ‚ö†Ô∏è Reminder\n",
    "It's important to keep in mind that we use `.fit_transform()` in the training data.\n",
    "Then we use `.transform()` on other dataset splits (test or validation).\n",
    "Mistakenly applying `.fit_transform()` on the test data will run without raising any errors, but may result in inconsistent transformations between different dataset splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefd4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559332c",
   "metadata": {},
   "source": [
    "#### üìù Note\n",
    "\n",
    "Obviously, not every document contains all words present in other documents from the same corpus.\n",
    "This results in sparse TF-IDF vectors (i.e. vectors that contain many zero elements).\n",
    "Storing this data in memory as-is would not be very efficient.\n",
    "\n",
    "Thankfully, `numpy` can efficiently store only non-zero elements via a data structure called a **sparse matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "314c35f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15368x45755 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1278358 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb83590",
   "metadata": {},
   "source": [
    "This data sparsity also results in a very high-dimensional feature space.\n",
    "E.g. if we initialize our vectorizer as `TfidfVectorizer(max_df=0.9)`, the number of features is a whopping 45755 !\n",
    "\n",
    "Changing the hyperparameters of `TfidfVectorizer` will give you a different number of features.\n",
    "The trade-off is between 1) the number of features you'd like to keep and 2) sparsity of the data\n",
    "\n",
    "The optimal parameters depend on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "technological-symbol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15368, 45755)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08c0672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5123, 45755)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f94308",
   "metadata": {},
   "source": [
    "#### Baseline performance\n",
    "Every attempt to build a predictive model should always start with estimating the baseline.\n",
    "It can mean different things depending on the problem:\n",
    "- known benchmarks of human performance\n",
    "- results achieved by others on the same dataset (e.g. academic literature, competition platforms like Kaggle)\n",
    "- existing rule-based system\n",
    "- etc.\n",
    "\n",
    "In the absence of the above, the following can provide the \"worst-case\" baseline predictions:\n",
    "- for regression tasks: average value of the target variable\n",
    "- for classification tasks: most common class in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "flush-alliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946142882361142"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred_baseline = [y_test.mean()]*len(y_test)\n",
    "baseline_mae = mean_absolute_error(y_test, y_pred_baseline)\n",
    "baseline_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641773f",
   "metadata": {},
   "source": [
    "Now that we have our \"worst-case\" baseline MAE.\n",
    "Let's see if a simple `LinearRegression` model can give us better results.\n",
    "We follow a standard 5-step process for training a model using `sklearn`'s API:\n",
    "1. import the model from it's respective module in `sklearn`\n",
    "2. create an instance of the model\n",
    "3. train the model by passing feature and target arrays to the `.fit()` method\n",
    "4. generate test predictions by passing test feature arrays to the `.predict()` method\n",
    "5. compute your metric(s) of choice by passing your true and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vocal-fossil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8932097643539854"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_tfidf, y_train)\n",
    "y_pred = reg.predict(X_test_tfidf)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537d007",
   "metadata": {},
   "source": [
    "`LinearRegression` gave us a slightly better result (i.e. lower error).\n",
    "What about a more powerful model like gradient boosting regression trees `XGBRegressor` from the `xgboost` library?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "secondary-cathedral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611975399540963"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "reg = XGBRegressor()\n",
    "reg.fit(X_train_tfidf, y_train)\n",
    "y_pred = reg.predict(X_test_tfidf)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864c908",
   "metadata": {},
   "source": [
    "Now that's a more substantial improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d0185e",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Your turn: transform text with Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c53a77",
   "metadata": {},
   "source": [
    "Your task is to:\n",
    "1. transform both train and test datasets using `CountVectorizer` from sklearn\n",
    "2. train a `LinearRegression` and evaluate its performance\n",
    "3. train a `XGBRegressor` and evaluate its performance\n",
    "4. write conclusions comparing `CountVectorizer` and `TfidfVectorizer` after training two types of regressions models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a425a72",
   "metadata": {},
   "source": [
    "#### üìù Note\n",
    "\n",
    "Count vectorizer is very similar to TF-IDF vectorizer, except for one major difference:\n",
    "instead of computing a TF-IDF score for each word, it's simply counts the number of occurrences  of that word in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e7dbce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A SOLUTION: REMOVE BEFORE THE PRESENTATION\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b8267b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A SOLUTION: REMOVE BEFORE THE PRESENTATION\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "067fa74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6735846665074579"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS IS A SOLUTION: REMOVE BEFORE THE PRESENTATION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_count, y_train)\n",
    "y_pred = reg.predict(X_test_count)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "728fe39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6096818648829908"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS IS A SOLUTION: REMOVE BEFORE THE PRESENTATION\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "reg = XGBRegressor()\n",
    "reg.fit(X_train_count, y_train)\n",
    "y_pred = reg.predict(X_test_count)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba70c9f",
   "metadata": {},
   "source": [
    "#### Write your conclusions here:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2f5b7",
   "metadata": {},
   "source": [
    "### Transform text with word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12902a",
   "metadata": {},
   "source": [
    "#### üìù Note\n",
    "\n",
    "As a quick refresher on word2vec:\n",
    "- It's a technique that trains a shallow neural network that 1) takes a word from a document as input and 2) predicts a few words (typically 2-3 words) before and after the input word. I.e. the model learns to predict contextual words\n",
    "- After being trained on a large number of documents, this neural network learns how to encode any word as a dense vector (as opposed to a sparse vector produced by techniques like TF-IDF or Count Vectorizer)\n",
    "- In dense vectors, each dimension no longer corresponds to an individual word. These dimensions encode some latent meanings and aren't easily interpretable by us, humans\n",
    "- Dense vectors have much lower dimensionality. Depending on the model being used, the dimensionality can range between ~100-2000. That's much lower than the 45000 dimensions produced by TF-IDF!\n",
    "\n",
    "\n",
    "How do you encode an entire document, not just individual words?\n",
    "The library [spacy](https://spacy.io/) that we are using below takes a simple approach:\n",
    "- encode individual words as dense vectors\n",
    "- take the average of all dense vectors to produce the one that'll represent the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "smaller-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize data using word2vec \n",
    "# https://spacy.io/usage/linguistic-features#vectors-similarity\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def get_spacy_vec(text):\n",
    "    doc = nlp(text)\n",
    "    return doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24070a5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.30199000e-01,  1.80031300e-01, -1.36240199e-01, -2.45516952e-02,\n",
       "       -2.30916012e-02,  1.20573625e-01,  6.47889525e-02, -7.64670223e-03,\n",
       "        1.53280739e-02,  1.79247594e+00, -1.48580819e-01, -4.93610874e-02,\n",
       "        2.30338618e-01,  5.89996800e-02,  2.06603985e-02,  5.95003441e-02,\n",
       "       -1.24250026e-02,  1.42414904e+00, -1.47959694e-01, -1.52537793e-01,\n",
       "       -4.64773029e-02, -1.00600198e-01, -1.36919022e-02,  4.05407958e-02,\n",
       "        3.48809967e-03,  1.05899200e-01, -6.18040096e-03,  1.12988949e-02,\n",
       "        1.34248301e-01, -2.80837975e-02, -1.48707196e-01,  1.34261996e-01,\n",
       "        1.27721876e-01, -6.22820966e-02,  4.02175169e-03, -2.76759872e-03,\n",
       "        9.70945507e-02, -1.32355958e-01, -2.46288180e-01, -1.24164663e-01,\n",
       "        7.99608007e-02,  4.09421399e-02,  6.55909032e-02, -1.29649490e-01,\n",
       "        1.37287989e-01, -5.05333059e-02, -6.37686029e-02,  6.17356896e-02,\n",
       "        1.64647788e-01, -3.23910080e-02, -2.03101993e-01,  6.60973042e-02,\n",
       "        3.81690566e-03,  1.13390669e-01,  6.17213026e-02, -2.27554999e-02,\n",
       "        7.31653571e-02,  1.02817796e-01, -1.00194998e-02, -2.26017907e-01,\n",
       "        1.05003072e-02,  5.97418919e-02, -6.89650029e-02,  2.02645987e-01,\n",
       "        5.30769005e-02, -3.66232991e-02, -3.18422318e-01,  1.46734998e-01,\n",
       "        1.26884639e-01,  1.59209207e-01,  1.59153625e-01,  1.74770206e-01,\n",
       "        2.27243707e-01,  5.70322052e-02,  7.73051083e-02, -1.69350415e-01,\n",
       "       -1.28498346e-01, -1.43459395e-01, -2.08329439e-01,  1.70272306e-01,\n",
       "       -1.79927964e-02,  2.40887120e-01, -7.60427043e-02, -8.30394961e-03,\n",
       "        3.35142314e-01, -2.39422485e-01,  1.22769000e-02, -2.89240301e-01,\n",
       "        1.19446991e-02,  3.17976028e-02, -1.41665190e-01,  4.15303618e-01,\n",
       "        9.71219502e-03,  3.87137942e-02, -9.23103020e-02,  7.49192014e-02,\n",
       "       -1.73420995e-01, -8.74616951e-02,  6.79358989e-02,  7.54238963e-02,\n",
       "        3.20025049e-02, -4.38582152e-03, -9.72988009e-02,  1.86111823e-01,\n",
       "        3.23664278e-01, -6.40780449e-01,  2.51428515e-01,  4.75042984e-02,\n",
       "        5.40939858e-03,  9.41197039e-04,  5.99605963e-02, -1.00380182e-03,\n",
       "        1.58291906e-01,  1.04935512e-01, -1.09140255e-01,  1.58663988e-01,\n",
       "       -1.97850317e-01,  2.12461874e-01, -4.52192016e-02,  1.57077964e-02,\n",
       "        8.49329978e-02, -3.65101993e-02,  7.47229010e-02,  7.20360037e-03,\n",
       "       -1.58421397e-01,  1.43037200e-01, -1.46054700e-01, -2.18291000e-01,\n",
       "       -8.43342990e-02, -2.95895152e-02, -8.15806016e-02, -5.99159971e-02,\n",
       "        8.03116933e-02, -3.96016985e-02,  6.88525066e-02,  1.57682389e-01,\n",
       "       -4.85980511e-03,  2.19632387e-01,  1.52936697e-01,  3.89752388e-02,\n",
       "       -1.20318389e+00,  8.02142993e-02, -2.15466972e-02, -9.23376083e-02,\n",
       "       -3.25967036e-02, -1.36801198e-01, -1.63841605e-01,  6.81115016e-02,\n",
       "        4.29720953e-02,  7.20392913e-02, -1.20200992e-01,  1.06172465e-01,\n",
       "       -7.61782825e-02, -9.32139009e-02,  2.02994011e-02, -2.81794846e-01,\n",
       "       -1.26464725e-01, -1.22230791e-01,  9.50699765e-03, -7.76186064e-02,\n",
       "        1.12021044e-02,  3.25739942e-02, -5.73131964e-02, -4.45922017e-02,\n",
       "       -2.96973884e-01, -1.10818602e-01,  6.14874959e-02, -3.14816460e-02,\n",
       "        1.18273281e-01,  1.54221296e-01,  1.08646616e-01, -8.54171067e-02,\n",
       "        8.38995054e-02, -2.34074995e-01, -1.86673090e-01, -3.51279005e-02,\n",
       "        1.41311198e-01, -4.86885011e-02,  3.11311129e-02,  9.39125847e-03,\n",
       "       -1.46361202e-01, -8.85150023e-03,  8.72619972e-02, -6.75871074e-02,\n",
       "        8.96149874e-03,  6.78989897e-03, -1.90062985e-01, -3.23767029e-02,\n",
       "       -9.34178531e-02, -1.44660190e-01, -1.28989503e-01, -8.49553943e-02,\n",
       "       -2.43988395e-01,  4.88338992e-02,  2.60012418e-01,  2.48834461e-01,\n",
       "       -7.86018968e-02, -7.56814033e-02,  7.01194978e-04,  6.65449873e-02,\n",
       "       -2.46965379e-01, -1.43147096e-01, -1.27126798e-01,  1.14810407e-01,\n",
       "        1.47422597e-01, -2.82333996e-02,  1.57831877e-01,  1.25618488e-01,\n",
       "        1.41111668e-02, -2.74690054e-02, -3.02196033e-02, -1.08029604e-01,\n",
       "        2.20582001e-02, -1.12893403e-01,  2.12030057e-02,  2.83904970e-02,\n",
       "       -3.14266905e-02,  1.25459298e-01, -2.99355567e-01,  1.12193801e-01,\n",
       "       -2.84196027e-02, -7.55160004e-02, -1.16896197e-01, -2.67153941e-02,\n",
       "        4.05178778e-02,  5.02207829e-03,  1.20222092e-01,  1.18845902e-01,\n",
       "        1.05773054e-01,  1.14875391e-01, -2.98728168e-01,  1.28822312e-01,\n",
       "       -1.02307156e-01,  1.17992781e-01, -1.68949097e-01,  4.08038497e-02,\n",
       "       -4.53287475e-02, -7.81032890e-02, -4.26770523e-02,  6.36684895e-02,\n",
       "        7.12945014e-02,  3.47240008e-02,  9.75503922e-02, -2.41503995e-02,\n",
       "        9.34463888e-02, -1.50848120e-01,  1.39019057e-01, -4.03669029e-02,\n",
       "       -1.22474298e-01,  1.74721032e-02, -3.41012999e-02,  5.69537990e-02,\n",
       "        1.20906517e-01, -1.38650879e-01,  7.35805035e-02,  2.40822986e-01,\n",
       "        1.18314996e-02,  2.23041046e-02,  1.05819598e-01, -9.12981108e-02,\n",
       "       -5.93839679e-03,  5.74986935e-02, -1.12532496e-01,  6.08235970e-02,\n",
       "       -1.21206017e-02,  1.01042107e-01,  1.12526998e-01,  1.35693997e-01,\n",
       "        7.63032973e-01,  2.79556997e-02, -1.23045005e-01, -6.32550046e-02,\n",
       "       -8.33565965e-02, -1.58759594e-01, -8.14720429e-03, -1.02722906e-01,\n",
       "       -6.51585013e-02, -6.72800094e-03,  7.86469691e-03,  2.47887418e-01,\n",
       "        2.85236508e-01,  2.68888086e-01,  1.70480404e-02, -1.88074604e-01,\n",
       "       -4.96296324e-02, -8.29416066e-02,  1.15097798e-01,  2.34352984e-02,\n",
       "        8.76208916e-02, -1.80947676e-01, -4.09017988e-02,  3.10928039e-02,\n",
       "        5.53795919e-02, -1.63461417e-01,  3.91885415e-02, -2.15580791e-01,\n",
       "       -1.71443492e-01, -5.43079991e-03, -5.86769991e-02,  1.58194005e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = get_spacy_vec(\"Here's the vector that encodes this sample sentence!\")\n",
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b9cc3",
   "metadata": {},
   "source": [
    "The vector has only 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a3ccc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fa8bb",
   "metadata": {},
   "source": [
    "Let's transform our text to dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c898c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15368/15368 [04:04<00:00, 62.77it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5123/5123 [01:18<00:00, 65.65it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_emb = X_train.progress_apply(get_spacy_vec)\n",
    "X_train_emb = np.array(X_train_emb.to_list())\n",
    "\n",
    "X_test_emb = X_test.progress_apply(get_spacy_vec)\n",
    "X_test_emb = np.array(X_test_emb.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f24e8c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15368, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b8fe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5123, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "photographic-sweden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6365379829926112"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train_emb, y_train)\n",
    "y_pred = reg.predict(X_test_emb)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "naughty-underground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6100463889798842"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = XGBRegressor()\n",
    "reg.fit(X_train_emb, y_train)\n",
    "y_pred = reg.predict(X_test_emb)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d7bb5",
   "metadata": {},
   "source": [
    "Look how small the performance gap is between a simple `LinearRegression` model and an advanced `XGBRegressor` model!\n",
    "\n",
    "That's the power of `word2vec`.\n",
    "\n",
    "NLP practitioners often refer to `word2vec` as \"sriracha sauce of NLP\" üå∂Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f3182",
   "metadata": {},
   "source": [
    "### Analysis of residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7f930",
   "metadata": {},
   "source": [
    "No regression modelling task is complete without properly analyzing the residuals.\n",
    "Is there any bias towards over/under predicting the rating?\n",
    "\n",
    "If the model is good:\n",
    "- the mean of the residuals should be close to 0\n",
    "- the standard deviation of the residuals should be small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a675498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.019376390054967402, 0.8037757131986021)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid = y_test - y_pred\n",
    "resid.mean(), resid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9ac29a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the distribution of the residuals\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(resid, bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28742413",
   "metadata": {},
   "source": [
    "#### üìù Note\n",
    "\n",
    "We haven't done any hyper-parameter tuning, so everything we've done so far can be improved if we picked a better set of hyper-parameters.\n",
    "Here are some homework ideas for you to try:\n",
    "- Explore the hyper-parameters of both:\n",
    "   - [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "   - [`XGBRegressor`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)\n",
    "- Pick a subset of hyper-parameters and tune them using either:\n",
    "   - [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "   - [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-graphic",
   "metadata": {},
   "source": [
    "# ‚úîÔ∏èSummary\n",
    "- `TF-IDF` is much faster than `word2vec`\n",
    "- `XGBRegressor` significantly outperforms `LinearRegression` when text is vectorized using `TF-IDF`\n",
    "- The performance of `LinearRegression` improves significantly when text is vectorized using `word2vec` rather than `TF-IDF`\n",
    "- The performance gap between `XGBRegressor` and `LinearRegression` is small when using `word2vec`. The important point here is that a powerful text vectorization technique can lead to good results even when using simple algorithms like linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d31ea1",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "- [Course: Advanced NLP with spacy](https://course.spacy.io/en/)\n",
    "- [Video: TF-IDF Document Similarity using Cosine Similarity](https://www.youtube.com/watch?v=hc3DCn8viWs)\n",
    "- [Video: Understanding Word Embeddings 2: CBOW and Skip Gram](https://www.youtube.com/watch?v=BWaHLmG1lak)\n",
    "- [Video: Word Vector Representations word2vec](https://www.youtube.com/watch?v=ERibwqs9p38)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-webinar)",
   "language": "python",
   "name": "nlp-webinar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
